<h1>Documentation</h1>

<p>The relational data is internally transformed into structured JSON objects for the schema-less
indexing model of Elasticsearch documents.</p>

<p>The importer can fetch data from RDBMS while multithreaded bulk mode ensures high throughput when 
indexing to Elasticsearch.</p>

<h2>JDBC importer definition file</h2>

<p>The general form of a JDBC import specification is a JSON object.</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
         &lt;definition&gt;
    }
}</code></pre>

<p>Example:</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : &quot;select * from orders&quot;,
        &quot;index&quot; : &quot;myindex&quot;,
        &quot;type&quot; : &quot;mytype&quot;,
        ...	         
    }
}</code></pre>

<p>The importer can either be executed via stdin (for example with echo)</p>

<pre><code>bin=$JDBC_IMPORTER_HOME/bin
lib=$JDBC_IMPORTER_HOME/lib
echo &#39;{
  ...
}&#39; | java \
	-cp &quot;${lib}/*&quot; \
	-Dlog4j.configurationFile=${bin}/log4j2.xml \
	org.xbib.tools.Runner \
	org.xbib.tools.JDBCImporter</code></pre>

<p>or with explicit file name parameter from command line. Here is an example
where <code>statefile.json</code> is a file which is loaded before execution.</p>

<pre><code>java \
	-cp &quot;${lib}/*&quot; \
	-Dlog4j.configurationFile=${bin}/log4j2.xml \
	org.xbib.tools.Runner \
	org.xbib.tools.JDBCImporter \
	statefile.json</code></pre>

<p>This style is convenient for subsequent execution controlled by the <code>statefile</code> parameter
if <code>statefile</code> is set to <code>statefile.json</code>.</p>

<h3>Parameters</h3>

<p>Here is the list of parameters for the <code>jdbc</code> block in the definition.</p>

<p><code>strategy</code> - the strategy of the JDBC importer, currently implemented: <code>&quot;standard&quot;</code>, <code>&quot;column&quot;</code></p>

<p><code>url</code> - the JDBC driver URL</p>

<p><code>user</code> - the JDBC database user</p>

<p><code>password</code> - the JDBC database password</p>

<p><code>sql</code>  - SQL statement(s), either a string or a list. If a statement ends with .sql, the statement is looked up in the file system. Example for a list of SQL statements:</p>

<pre><code>&quot;sql&quot; : [
    {
        &quot;statement&quot; : &quot;select ... from ... where a = ?, b = ?, c = ?&quot;,
        &quot;parameter&quot; : [ &quot;value for a&quot;, &quot;value for b&quot;, &quot;value for c&quot; ]
    },
    {
        &quot;statement&quot; : &quot;insert into  ... where a = ?, b = ?, c = ?&quot;,
        &quot;parameter&quot; : [ &quot;value for a&quot;, &quot;value for b&quot;, &quot;value for c&quot; ],
        &quot;write&quot; : &quot;true&quot;
    },
    {
        &quot;statement&quot; : ...
    }
]</code></pre>

<p><code>sql.statement</code> - the SQL statement</p>

<p><code>sql.write</code> - boolean flag, if true, the SQL statement is interpreted as an insert/update statement that needs write access (default: false).</p>

<p><code>sql.callable</code> - boolean flag, if true, the SQL statement is interpreted as a JDBC <code>CallableStatement</code> for stored procedures (default: false).</p>

<p><code>sql.parameter</code> - bind parameters for the SQL statement (in order). Some special values can be used with the following meanings:</p>

<ul><li><code>$now</code> - the current timestamp</li><li><code>$state</code> - the state, one of: BEFORE<em>FETCH, FETCH, AFTER</em>FETCH, IDLE, EXCEPTION</li><li><code>$metrics.counter</code> - a counter</li><li><code>$lastrowcount</code> - number of rows from last statement</li><li><code>$lastexceptiondate</code> - SQL timestamp of last exception</li><li><code>$lastexception</code> - full stack trace of last exception</li><li><code>$metrics.lastexecutionstart</code> - SQL timestamp of the time when last execution started</li><li><code>$metrics.lastexecutionend</code> - SQL timestamp of the time when last execution ended</li><li><code>$metrics.totalrows</code> - total number of rows fetched</li><li><code>$metrics.totalbytes</code> - total number of bytes fetched</li><li><code>$metrics.failed</code> - total number of failed SQL executions</li><li><code>$metrics.succeeded</code> - total number of succeeded SQL executions</li></ul>

<p><code>locale</code> - the default locale (used for parsing numerical values, floating point character. Recommended values is &quot;en_US&quot;)</p>

<p><code>timezone</code> - the timezone for JDBC setTimestamp() calls when binding parameters with timestamp values</p>

<p><code>rounding</code> -  rounding mode for parsing numeric values. Possible values  &quot;ceiling&quot;, &quot;down&quot;, &quot;floor&quot;, &quot;halfdown&quot;, &quot;halfeven&quot;, &quot;halfup&quot;, &quot;unnecessary&quot;, &quot;up&quot;</p>

<p><code>scale</code> -  the precision of parsing numeric values</p>

<p><code>autocommit</code> - <code>true</code> if each statement should be automatically executed. Default is <code>false</code></p>

<p><code>fetchsize</code> - the fetchsize for large result sets, most drivers use this to control the amount of rows in the buffer while iterating through the result set</p>

<p><code>max_rows</code> - limit the number of rows fetches by a statement, the rest of the rows is ignored</p>

<p><code>max_retries</code> - the number of retries to (re)connect to a database</p>

<p><code>max_retries_wait</code> - a time value for the time that should be waited between retries. Default is &quot;30s&quot;</p>

<p><code>resultset_type</code> - the JDBC result set type, can be TYPE<em>FORWARD</em>ONLY, TYPE<em>SCROLL</em>SENSITIVE, TYPE<em>SCROLL</em>INSENSITIVE. Default is TYPE<em>FORWARD</em>ONLY</p>

<p><code>resultset_concurrency</code> - the JDBC result set concurrency, can be CONCUR<em>READ</em>ONLY, CONCUR<em>UPDATABLE. Default is CONCUR</em>UPDATABLE</p>

<p><code>ignore_null_values</code> - if NULL values should be ignored when constructing JSON documents. Default is <code>false</code></p>

<p><code>detect_geo</code> - if geo polygons / points in SQL columns should be parsed when constructing JSON documents. Default is <code>true</code></p>

<p><code>detect_json</code> - if json structures in SQL columns should be parsed when constructing JSON documents. Default is <code>true</code></p>

<p><code>prepare_database_metadata</code> - if the driver metadata should be prepared as parameters.  Default is <code>false</code></p>

<p><code>prepare_resultset_metadata</code> - if the result set metadata should be prepared as parameters.  Default is <code>false</code></p>

<p><code>column_name_map</code> - a map of aliases that should be used as a replacement for column names of the database. Useful for Oracle 30 char column name limit. Default is <code>null</code></p>

<p><code>query_timeout</code> - a second value for how long an SQL statement is allowed to be executed before it is considered as lost. Default is <code>1800</code></p>

<p><code>connection_properties</code> - a map for the connection properties for driver connection creation. Default is <code>null</code></p>

<p><code>schedule</code> - a single or a list of cron expressions for scheduled execution. Syntax is equivalent to the
Quartz cron expression format (see below for syntax)</p>

<p><code>threadpoolsize</code> - a thread pool size for the scheduled executions for <code>schedule</code> parameter. If set to <code>1</code>, all jobs will be executed serially. Default is <code>4</code>.</p>

<p><code>interval</code> - a time value for the delay between two runs (default: not set)</p>

<p><code>elasticsearch.cluster</code> - Elasticsearch cluster name</p>

<p><code>elasticsearch.host</code> - array of Elasticsearch host specifications (host name or <code>host:port</code>)</p>

<p><code>elasticsearch.port</code> -  port of Elasticsearch host</p>

<p><code>elasticsearch.autodiscover</code> - if <code>true</code>, JDBC importer will try to connect to all cluster nodes. Default is <code>false</code></p>

<p><code>max_bulk_actions</code> - the length of each bulk index request submitted (default: 10000)</p>

<p><code>max_concurrent_bulk_requests</code> - the maximum number of concurrent bulk requests (default: 2 * number of CPU cores)</p>

<p><code>max_bulk_volume</code> - a byte size parameter for the maximum volume allowed for a bulk request (default: &quot;10m&quot;)</p>

<p><code>max_request_wait</code> - a time value for the maximum wait time for a response of a bulk request (default: &quot;60s&quot;)</p>

<p><code>flush_interval</code> - a time value for the interval period of flushing index docs to a bulk action (default: &quot;5s&quot;)</p>

<p><code>index</code> - the Elasticsearch index used for indexing</p>

<p><code>type</code> - the Elasticsearch type of the index used for indexing</p>

<p><code>index_settings</code> - optional settings for the Elasticsearch index</p>

<p><code>type_mapping</code> - optional mapping for the Elasticsearch index type</p>

<p><code>statefile</code> - name of a file where the JDBC importer reads or writes state information </p>

<p><code>metrics.lastexecutionstart</code> - the UTC date/time of the begin of the last execution of a single fetch</p>

<p><code>metrics.lastexecutionend</code> - the UTC date/time of the end of the last execution of a single fetch</p>

<p><code>metrics.counter</code> - a counter for metrics, will be incremented after each single fetch</p>

<p><code>metrics.enabled</code> - if <code>true</code>, metrics logging is enabled. Default is <code>false</code></p>

<p><code>metrics.interval</code> - the interval between metrics logging. Default is 30 seconds.</p>

<p><code>metrics.logger.plain</code> - if <code>true</code>, write metrics log messages in plain text format. Default is <code>false</code></p>

<p><code>metrics.logger.json</code> - if <code>true</code>, write metric log messages in JSON format. Default is <code>false</code></p>

<h2>Overview about the default parameter settings</h2>

<pre><code>{
    &quot;jdbc&quot; : {
		&quot;strategy&quot; : &quot;standard&quot;,
        &quot;url&quot; : null,
        &quot;user&quot; : null,
        &quot;password&quot; : null,
        &quot;sql&quot; : null,
        &quot;locale&quot; : /* equivalent to Locale.getDefault().toLanguageTag() */,
        &quot;timezone&quot; : /* equivalent to TimeZone.getDefault() */,
        &quot;rounding&quot; : null,
        &quot;scale&quot; : 2,
        &quot;autocommit&quot; : false,
        &quot;fetchsize&quot; : 10, /* if URL contains MySQL JDBC driver URL, this is Integer.MIN */
        &quot;max_rows&quot; : 0,
        &quot;max_retries&quot; : 3,
        &quot;max_retries_wait&quot; : &quot;30s&quot;,
        &quot;resultset_type&quot; : &quot;TYPE_FORWARD_ONLY&quot;,
        &quot;resultset_concurreny&quot; : &quot;CONCUR_UPDATABLE&quot;,
        &quot;ignore_null_values&quot; : false,
        &quot;prepare_database_metadata&quot; : false,
        &quot;prepare_resultset_metadata&quot; : false,
        &quot;column_name_map&quot; : null,
        &quot;query_timeout&quot; : 1800,
        &quot;connection_properties&quot; : null,
		&quot;schedule&quot; : null,
		&quot;interval&quot; : 0L,
		&quot;threadpoolsize&quot; : 1,
        &quot;index&quot; : &quot;jdbc&quot;,
        &quot;type&quot; : &quot;jdbc&quot;,
        &quot;index_settings&quot; : null,
        &quot;type_mapping&quot; : null,
		&quot;max_bulk_actions&quot; : 10000,
		&quot;max_concurrent_bulk_requests&quot; : 2 * available CPU cores,
		&quot;max_bulk_volume&quot; : &quot;10m&quot;,
		&quot;max_request_wait&quot; : &quot;60s&quot;,
		&quot;flush_interval&quot; : &quot;5s&quot;
    }
}</code></pre>

<h2>Time scheduled execution</h2>

<p>Setting a cron expression in the parameter <code>schedule</code> enables repeated (or time scheduled) runs.</p>

<p>You can also define a list of cron expressions (in a JSON array) to schedule for many
different time schedules.</p>

<p>Example of a <code>schedule</code> parameter:</p>

<pre><code>    &quot;schedule&quot; : &quot;0 0-59 0-23 ? * *&quot;</code></pre>

<p>This executes JDBC importer every minute, every hour, all the days in the week/month/year.</p>

<p>The following documentation about the syntax of the cron expression is copied from the Quartz 
scheduler javadoc page.</p>

<p>Cron expressions provide the ability to specify complex time combinations such as
&quot;At 8:00am every Monday through Friday&quot; or &quot;At 1:30am every last Friday of the month&quot;.</p>

<p>Cron expressions are comprised of 6 required fields and one optional field separated by
white space. The fields respectively are described as follows:</p>


<table class="table"><thead>
<tr>
<th>Field Name</th>
<th>Allowed Values</th>
<th>Allowed Special Characters</th>
</tr>
</thead><tbody>
<tr>
<td>Seconds</td>
<td>0-59</td>
<td>, - * /</td>
</tr>
<tr>
<td>Minutes</td>
<td>0-59</td>
<td>, - * /</td>
</tr>
<tr>
<td>Hours</td>
<td>0-23</td>
<td>, - * /</td>
</tr>
<tr>
<td>Day-of-month</td>
<td>1-31</td>
<td>, - * ? / L W</td>
</tr>
<tr>
<td>Month</td>
<td>1-12 or JAN-DEC</td>
<td>, - * /</td>
</tr>
<tr>
<td>Day-of-Week</td>
<td>1-7 or SUN-SAT</td>
<td>, - * ? / L #</td>
</tr>
<tr>
<td>Year (Optional)</td>
<td>empty, 1970-2199</td>
<td>, - * /</td>
</tr>
</tbody></table>

<h2>Structured objects</h2>

<p>One of the advantage of SQL queries is the join operation. From many tables, new tuples can be formed.</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : &quot;select \&quot;relations\&quot; as \&quot;_index\&quot;, orders.customer as \&quot;_id\&quot;, orders.customer as \&quot;contact.customer\&quot;, employees.name as \&quot;contact.employee\&quot; from orders left join employees on employees.department = orders.department order by _id&quot;
    }
}</code></pre>

<p>For example, these rows from SQL</p>

<pre><code>mysql&gt; select &quot;relations&quot; as &quot;_index&quot;, orders.customer as &quot;_id&quot;, orders.customer as &quot;contact.customer&quot;, employees.name as &quot;contact.employee&quot;  from orders left join employees on employees.department = orders.department order by _id;
+-----------+-------+------------------+------------------+
| _index    | _id   | contact.customer | contact.employee |
+-----------+-------+------------------+------------------+
| relations | Big   | Big              | Smith            |
| relations | Large | Large            | Müller           |
| relations | Large | Large            | Meier            |
| relations | Large | Large            | Schulze          |
| relations | Huge  | Huge             | Müller           |
| relations | Huge  | Huge             | Meier            |
| relations | Huge  | Huge             | Schulze          |
| relations | Good  | Good             | Müller           |
| relations | Good  | Good             | Meier            |
| relations | Good  | Good             | Schulze          |
| relations | Bad   | Bad              | Jones            |
+-----------+-------+------------------+------------------+
11 rows in set (0.00 sec)</code></pre>

<p>will generate fewer JSON objects for the index <code>relations</code>.</p>

<pre><code>index=relations id=Big {&quot;contact&quot;:{&quot;employee&quot;:&quot;Smith&quot;,&quot;customer&quot;:&quot;Big&quot;}}
index=relations id=Large {&quot;contact&quot;:{&quot;employee&quot;:[&quot;Müller&quot;,&quot;Meier&quot;,&quot;Schulze&quot;],&quot;customer&quot;:&quot;Large&quot;}}
index=relations id=Huge {&quot;contact&quot;:{&quot;employee&quot;:[&quot;Müller&quot;,&quot;Meier&quot;,&quot;Schulze&quot;],&quot;customer&quot;:&quot;Huge&quot;}}
index=relations id=Good {&quot;contact&quot;:{&quot;employee&quot;:[&quot;Müller&quot;,&quot;Meier&quot;,&quot;Schulze&quot;],&quot;customer&quot;:&quot;Good&quot;}}
index=relations id=Bad {&quot;contact&quot;:{&quot;employee&quot;:&quot;Jones&quot;,&quot;customer&quot;:&quot;Bad&quot;}}</code></pre>

<p>Note how the <code>employee</code> column is collapsed into a JSON array. The repeated occurrence of the <code>_id</code> column
controls how values are folded into arrays for making use of the Elasticsearch JSON data model. Make sure your SQL query is ordered by <code>_id</code>.</p>

<h2>Column names for JSON document construction</h2>

<p>In SQL, each column may be labeled. This label is used by the JDBC importer for JSON document
construction. The dot is the path separator for the document strcuture.</p>

<p>For example</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : &quot;select products.name as \&quot;product.name\&quot;, orders.customer as \&quot;product.customer.name\&quot;, orders.quantity * products.price as \&quot;product.customer.bill\&quot; from products, orders where products.name = orders.product&quot;
    }
}</code></pre>

<p>the labeled columns are <code>product.name</code>, <code>product.customer.name</code>, and <code>product.customer.bill</code>.</p>

<p>A data example:</p>

<pre><code>mysql&gt; select products.name as &quot;product.name&quot;, orders.customer as &quot;product.customer&quot;, orders.quantity * products.price as &quot;product.customer.bill&quot; from products, orders where products.name = orders.product ;
+--------------+------------------+-----------------------+
| product.name | product.customer | product.customer.bill |
+--------------+------------------+-----------------------+
| Apples       | Big              |                     1 |
| Bananas      | Large            |                     2 |
| Oranges      | Huge             |                     6 |
| Apples       | Good             |                     2 |
| Oranges      | Bad              |                     9 |
+--------------+------------------+-----------------------+
5 rows in set, 5 warnings (0.00 sec)</code></pre>

<p>The structured objects constructed from these columns are</p>

<pre><code>id=0 {&quot;product&quot;:{&quot;name&quot;:&quot;Apples&quot;,&quot;customer&quot;:{&quot;bill&quot;:1.0,&quot;name&quot;:&quot;Big&quot;}}}
id=1 {&quot;product&quot;:{&quot;name&quot;:&quot;Bananas&quot;,&quot;customer&quot;:{&quot;bill&quot;:2.0,&quot;name&quot;:&quot;Large&quot;}}}
id=2 {&quot;product&quot;:{&quot;name&quot;:&quot;Oranges&quot;,&quot;customer&quot;:{&quot;bill&quot;:6.0,&quot;name&quot;:&quot;Huge&quot;}}}
id=3 {&quot;product&quot;:{&quot;name&quot;:&quot;Apples&quot;,&quot;customer&quot;:{&quot;bill&quot;:2.0,&quot;name&quot;:&quot;Good&quot;}}}
id=4 {&quot;product&quot;:{&quot;name&quot;:&quot;Oranges&quot;,&quot;customer&quot;:{&quot;bill&quot;:9.0,&quot;name&quot;:&quot;Bad&quot;}}}</code></pre>

<p>There are column labels with an underscore as prefix that are mapped to special Elasticsearch document parameters for indexing:</p>

<pre><code>_index     the index this object should be indexed into
_type      the type this object should be indexed into
_id        the id of this object
_version   the version of this object
_parent    the parent of this object
_ttl       the time-to-live of this object
_routing   the routing of this object</code></pre>

<p>See also</p>

<p>http://www.elasticsearch.org/guide/reference/mapping/parent-field.html</p>

<p>http://www.elasticsearch.org/guide/reference/mapping/ttl-field.html</p>

<p>http://www.elasticsearch.org/guide/reference/mapping/routing-field.html</p>

<h2>Bracket notation for JSON array construction</h2>

<p>When construction JSON documents, it is often the case you want to group SQL columns into a JSON object and
line them up into JSON arrays. For allowing this, a bracket notation is used to identify children
elements that repeat in each child.</p>

<p>Note, because of limitations in identifying SQL column groups, nested document structures may lead to
repetitions of the same group. Fortunately, this is harmless to Elasticsearch queries.</p>

<p>Example:</p>

<table class="table"><thead>
<tr>
<th>_id</th>
<th>blog.name</th>
<th>blog.published</th>
<th>blog.association[id]</th>
<th>blog.association[name]</th>
<th>blog.attachment[id]</th>
<th>blog.attachment[name]</th>
</tr>
</thead><tbody>
<tr>
<td>4679</td>
<td>Joe</td>
<td>2014-01-06 00:00:00</td>
<td>3917</td>
<td>John</td>
<td>9450</td>
<td>/web/q/g/h/57436356.jpg</td>
</tr>
<tr>
<td>4679</td>
<td>Joe</td>
<td>2014-01-06 00:00:00</td>
<td>3917</td>
<td>John</td>
<td>9965</td>
<td>/web/i/s/q/GS3193626.jpg</td>
</tr>
<tr>
<td>4679</td>
<td>Joe</td>
<td>2014-01-06 00:00:00</td>
<td>3917</td>
<td>John</td>
<td>9451</td>
<td>/web/i/s/q/GS3193626.jpg</td>
</tr>
</tbody></table>

<pre><code>{
    &quot;blog&quot; : {
        &quot;attachment&quot;: [
            {
                &quot;name&quot; : &quot;/web/q/g/h/57436356.jpg&quot;,
                &quot;id&quot; : &quot;9450&quot;
            },
            {
                &quot;name&quot; : &quot;/web/i/s/q/GS3193626.jpg&quot;,
                &quot;id&quot; : &quot;9965&quot;
            },
            {
                &quot;name&quot; : &quot;/web/i/s/q/GS3193626.jpg&quot;,
                &quot;id&quot; : &quot;9451&quot;
            }
        ],
        &quot;name&quot; : &quot;Joe&quot;,
        &quot;association&quot; : [
            {
                &quot;name&quot; : &quot;John&quot;,
                &quot;id&quot; : &quot;3917&quot;
            },
            {
                &quot;name&quot; : &quot;John&quot;,
                &quot;id&quot; : &quot;3917&quot;
            },
            {
                &quot;name&quot; : &quot;John&quot;,
                &quot;id&quot; : &quot;3917&quot;
            }
         ],
         &quot;published&quot;:&quot;2014-01-06 00:00:00&quot;
     }
}</code></pre>

<h2>How to fetch a table?</h2>

<p>For fetching a table, a &quot;select *&quot; (star) query can be used.
Star queries are the simplest variant of selecting data from a database.
They dump tables into Elasticsearch row-by-row. If no <code>_id</code> column name is given, IDs will be automatically generated.</p>

<p>For example</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : &quot;select * from orders&quot;
    }
}</code></pre>

<p>and this table</p>

<pre><code>mysql&gt; select * from orders;
+----------+-----------------+---------+----------+---------------------+
| customer | department      | product | quantity | created             |
+----------+-----------------+---------+----------+---------------------+
| Big      | American Fruits | Apples  |        1 | 0000-00-00 00:00:00 |
| Large    | German Fruits   | Bananas |        1 | 0000-00-00 00:00:00 |
| Huge     | German Fruits   | Oranges |        2 | 0000-00-00 00:00:00 |
| Good     | German Fruits   | Apples  |        2 | 2012-06-01 00:00:00 |
| Bad      | English Fruits  | Oranges |        3 | 2012-06-01 00:00:00 |
+----------+-----------------+---------+----------+---------------------+
5 rows in set (0.00 sec)</code></pre>

<p>will result into the following JSON documents</p>

<pre><code>id=&lt;random&gt; {&quot;product&quot;:&quot;Apples&quot;,&quot;created&quot;:null,&quot;department&quot;:&quot;American Fruits&quot;,&quot;quantity&quot;:1,&quot;customer&quot;:&quot;Big&quot;}
id=&lt;random&gt; {&quot;product&quot;:&quot;Bananas&quot;,&quot;created&quot;:null,&quot;department&quot;:&quot;German Fruits&quot;,&quot;quantity&quot;:1,&quot;customer&quot;:&quot;Large&quot;}
id=&lt;random&gt; {&quot;product&quot;:&quot;Oranges&quot;,&quot;created&quot;:null,&quot;department&quot;:&quot;German Fruits&quot;,&quot;quantity&quot;:2,&quot;customer&quot;:&quot;Huge&quot;}
id=&lt;random&gt; {&quot;product&quot;:&quot;Apples&quot;,&quot;created&quot;:1338501600000,&quot;department&quot;:&quot;German Fruits&quot;,&quot;quantity&quot;:2,&quot;customer&quot;:&quot;Good&quot;}
id=&lt;random&gt; {&quot;product&quot;:&quot;Oranges&quot;,&quot;created&quot;:1338501600000,&quot;department&quot;:&quot;English Fruits&quot;,&quot;quantity&quot;:3,&quot;customer&quot;:&quot;Bad&quot;}</code></pre>

<h2>How to update a table?</h2>

<p>The JDBC importer allows to write data into the database for maintenance purpose. </p>

<p>Writing back data into the database makes sense for acknowledging fetched data. </p>

<p>Example:</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : [
            {
                &quot;statement&quot; : &quot;select * from \&quot;products\&quot;&quot;
            },
            {
                &quot;statement&quot; : &quot;delete from \&quot;products\&quot; where \&quot;_job\&quot; = ?&quot;,
                &quot;parameter&quot; : [ &quot;$job&quot; ]
            }
        ],
        &quot;index&quot; : &quot;my_jdbc_index&quot;,
        &quot;type&quot; : &quot;my_jdbc_type&quot;
    }
}</code></pre>

<p>In this example, the DB administrator has prepared product rows and attached a <code>_job</code> column to it
to enumerate the product updates incrementally. The assertion is that Elasticsearch should 
delete all products from the database after they are indexed successfully. The parameter <code>$job</code>
is a counter. The importer state is saved in a file, so the counter is persisted.</p>

<h2>How to select incremental data from a table?</h2>

<p>It is recommended to use timestamps in UTC for synchronization. This example fetches
all product rows which has added since the last run, using a millisecond resolution
column <code>mytimestamp</code>:</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;statefile&quot; : &quot;statefile.json&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : [
            {
                &quot;statement&quot; : &quot;select * from products where mytimestamp &gt; ?&quot;,
                &quot;parameter&quot; : [ &quot;$metrics.lastexecutionstart&quot; ]
            }
        ],
        &quot;index&quot; : &quot;my_jdbc_index&quot;,
        &quot;type&quot; : &quot;my_jdbc_type&quot;
    }
}</code></pre>

<p>the first time you run the script, it will generate the statefile.json file like this
<code>
{
  &quot;type&quot; : &quot;jdbc&quot;,
  &quot;jdbc&quot; : { 
    &quot;password&quot; : &quot;&quot;,
    &quot;index&quot; : &quot;my_jdbc_index&quot;,
    &quot;statefile&quot; : &quot;statefile.json&quot;,
    &quot;metrics&quot; : { 
      &quot;lastexecutionstart&quot; : &quot;2016-03-27T06:37:09.165Z&quot;,
      &quot;lastexecutionend&quot; : &quot;2016-03-27T06:37:09.501Z&quot;,
      &quot;counter&quot; : &quot;1&quot; 
    },  
    &quot;type&quot; : &quot;my_jdbc_type&quot;,
    &quot;user&quot; : &quot;&quot;,
    &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
    &quot;sql&quot; : [ { 
      &quot;statement&quot; : &quot;select * from products where mytimestamp &gt; ?&quot;, 
      &quot;parameter&quot; : [ &quot;$metrics.lastexecutionstart&quot; ]
    } ] 
  }
}
</code>
after this, you can select incremental data from table.</p>

<h2>Stored procedures or callable statements</h2>

<p>Stored procedures can also be used for fetchng data, like this example fo MySQL illustrates. 
See also <a href="http://docs.oracle.com/javase/tutorial/jdbc/basics/storedprocedures.html">Using Stored Procedures</a>
from where the example is taken.</p>

<pre><code>create procedure GET_SUPPLIER_OF_COFFEE(
    IN coffeeName varchar(32), 
    OUT supplierName varchar(40)) 
    begin 
        select SUPPLIERS.SUP_NAME into supplierName 
        from SUPPLIERS, COFFEES 
        where SUPPLIERS.SUP_ID = COFFEES.SUP_ID 
        and coffeeName = COFFEES.COF_NAME; 
        select supplierName; 
    end</code></pre>

<p>Now it is possible to call the procedure from the JDBC importer and index the result in Elasticsearch.</p>

<pre><code>{
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;sql&quot; : [
            {
                &quot;callable&quot; : true,
                &quot;statement&quot; : &quot;{call GET_SUPPLIER_OF_COFFEE(?,?)}&quot;,
                &quot;parameter&quot; : [
                     &quot;Colombian&quot;
                ],
                &quot;register&quot; : {
                     &quot;mySupplierName&quot; : { &quot;pos&quot; : 2, &quot;type&quot; : &quot;varchar&quot; }
                }
            }
        ],
        &quot;index&quot; : &quot;my_jdbc_index&quot;,
        &quot;type&quot; : &quot;my_jdbc_type&quot;
    }
}</code></pre>

<p>Note, the <code>parameter</code> lists the input parameters in the order they should be applied, like in an
ordinary statement. The <code>register</code> declares a list of output parameters in the particular order
the <code>pos</code> number indicates. It is required to declare the JDBC type in the <code>type</code> attribute.
<code>mySupplierName</code>, the key of the output parameter, is used as the Elasticsearch field name specification,
like the column name specification in an ordinary SQL statement, because column names are not available
in callable statement result sets.</p>

<p>If there is more than one result sets returned by a callable statement,
the JDBC importer enters a loop and iterates through all result sets.</p>

<h1>How to import from a CSV file?</h1>

<p>Importing from a CSV is easy because a CSV JDBC driver is included.</p>

<p>Try something like this</p>

<pre><code>{
	&quot;type&quot; : &quot;jdbc&quot;,
	&quot;jdbc&quot; : {
		&quot;driver&quot; : &quot;org.xbib.jdbc.csv.CsvDriver&quot;,
		&quot;url&quot; : &quot;jdbc:xbib:csv:mydatadir?columnTypes=&amp;separator=,&quot;,
		&quot;user&quot; : &quot;&quot;,
		&quot;password&quot; : &quot;&quot;,
		&quot;sql&quot; : &quot;select * from mycsvfile&quot;
	}
}</code></pre>

<p>where</p>

<p><code>mydatadir</code> - path to the directory where the CSV file exists</p>

<p><code>mycsvfile</code> - the name of the file</p>

<p><code>columnTypes</code> - column types will be inferred. Default is <code>String</code>, where column types will be all set to  string</p>

<p><code>separator</code> - the column separator</p>

<p>For a full list of the CSV JDBC driver options, see
https://github.com/jprante/jdbc-driver-csv</p>

<h1>Persisted state</h1>

<p>The JDBC importer writes the state after each execution step into a state file which can be set by the
parameter <code>statefile</code>, see above in the parameter documentation. Default setting is not writing 
to state file.</p>

<p>Example:</p>

<pre><code>&quot;sql&quot; : ...,
&quot;statefile&quot; : &quot;statefile.json&quot;,
...</code></pre>

<p>You can use the <code>statefile</code> as input for a next JDBC importer invocation, once it is saved. 
This is useful if you have to restart the JDBC importer. Because the statefile is written
in prettified JSON, it is also possible to adjust the 
settings in the statefile if you need to synchronize with the JDBC source.</p>

<p>Note: there must be enough space on disk to write the state file. If disk is full,
JDBC importer will write zero length files and give error messages in the importer log.</p>

<h1>Monitoring the JDBC importer</h1>

<p>Metrics logging can be enabled to watch for the current transfer statistics. </p>

<p>Example:</p>

<pre><code>&quot;sql&quot; : ...,
&quot;schedule&quot; : ...,
&quot;statefile&quot; : &quot;statefile.json&quot;,
&quot;metrics&quot; : {
    &quot;enabled&quot; : true,
    &quot;interval&quot; : &quot;1m&quot;,
    &quot;logger&quot; : {
        &quot;plain&quot; : false,
        &quot;json&quot; : true
    }
}</code></pre>

<p>This configuration enables metrics logging, sets the metrics logging interval to one minute,
and switches form plain loggin to JSON logging.</p>

<p>In the <code>log4j2.xml</code> configuration file, you can set up how to log. The loggers used for metrics logging are</p>

<p><code>metrics.source.plain</code> - for plain format logging of the source</p>

<p><code>metrics.sink.plain</code> - for plain format logging of the sink</p>

<p><code>metrics.source.json</code> - for JSON format logging of the source</p>

<p><code>metrics.sink.json</code> - for JSON format logging of the sink</p>

<p>See also the parameter documentation above.</p>

<h1>Developer notes</h1>

<h2>Source, Sink, Context</h2>

<p>The JDBC importer consists of three conceptual interfaces than can be implemented separately.</p>

<p>When you use the <code>strategy</code> parameter, the JDBC importer tries to load additional classes before
falling back to the <code>standard</code> strategy.</p>

<p>You can implement your own strategy by adding your implementation jars to the lib folder and
declaring the implementing classes in the <code>META-INF/services</code> directory. </p>

<p>So, it is easy to reuse or replace existing code, or adapt your own JDBC retrieval strategy
to the unmodified JDBC importer jar.</p>

<h3>Source</h3>

<p>The <code>Source</code> models the data producing side. Beside defining the JDBC connect parameters, 
it manages a dual-channel connection to the data producer for reading and for writing.
The reading channel is used for fetching data, while the writing channel can update the source.</p>

<p>The <code>Source</code> API can be inspected at 
http://jprante.github.io/elasticsearch-jdbc/apidocs/org/xbib/elasticsearch/jdbc/strategy/Source.html</p>

<h3>Sink</h3>

<p>The <code>Sink</code> is the abstraction of the destination where all the data is flowing from the source. 
It controls the resource usage of the bulk indexing method of Elasticsearch. T
hrottling is possible by limiting the number of bulk actions per request or by the 
maximum number of concurrent request.</p>

<p>The <code>Sink</code> API can be inspected at 
http://jprante.github.io/elasticsearch-jdbc/apidocs/org/xbib/elasticsearch/jdbc/strategy/Sink.html</p>

<h3>Context</h3>

<p>The <code>Context</code> is the abstraction to the thread which performs data fetching from the source 
and transports it to the mouth. A &#39;move&#39; is considered a single step in the execution cycle. </p>

<p>The <code>Context</code> API can be inspected at 
http://jprante.github.io/elasticsearch-jdbc/apidocs/org/xbib/elasticsearch/jdbc/strategy/Context.html</p>

<h2>Strategies</h2>

<p>The JDBC importer can be configured for different methods of data transport.
Such methods of data transports are called a &#39;strategy&#39;.</p>

<p>By default, the JDBC importer implements a <code>standard</code> strategy.</p>

<h2>Standard strategy</h2>

<p>The standard strategy contains the following steps of processing:</p>

<ol><li>fetch data from the JDBC connection</li><li>build structured objects and move them to Elasticsearch for indexing or deleting</li></ol>

<p>In the <code>sql</code> parameter, a series of SQL statements can be defined which are executed to fetch the data.</p>

<h2>Your custom strategy</h2>

<p>If you want to extend the JDBC importer, for example by your custom password authentication, you could
extend <code>org.xbib.elasticsearch.jdbc.strategy.standard.StandardSource</code>. 
Then, declare your strategy classes in <code>META-INF/services</code>. Add your
jar to the classpath and add the <code>strategy</code> parameter to the specifications.</p>

<h1>Examples</h1>

<h2>PostgreSQL</h2>

<ol><li><p>Install PostgreSQL</p><p>Example: PostgreSQL .dmg (Version 9.1.5) for Mac OS X from http://www.enterprisedb.com/products-services-training/pgdownload</p><p>Filename: postgresql-9.1.5-1-osx.dmg</p></li><li><p>Install Elasticsearch</p><p>Follow instructions on https://www.elastic.co/products/elasticsearch</p></li><li><p>Install JDBC importer</p><p><code>wget http://xbib.org/repository/org/xbib/elasticsearch/importer/elasticsearch-jdbc/&lt;version&gt;/elasticsearch-jdbc-&lt;version&gt;-dist.zip</code></p><p>(update version respectively)</p></li><li><p>Download PostgreSQL JDBC driver</p><p>Check http://jdbc.postgresql.org/download.html</p><p>Current version is JDBC4 Postgresql Driver, Version 9.1-902</p><p>Filname postgresql-9.1-902.jdbc4.jar</p></li><li><p>Copy driver into lib folder</p><pre><code>cp postgresql-9.1-902.jdbc4.jar $JDBC_IMPORTER_HOME/lib</code></pre></li><li><p>Start Elasticsearch</p></li><li><p>Start JDBC importer</p><p>This is just a basic example to a database <code>test</code> with user <code>fred</code> and password <code>secret</code>.
Use the port configured during PostgreSQL installation. The default is <code>5432</code>.
<code>
bin=$JDBC_IMPORTER_HOME/bin
lib=$JDBC_IMPORTER_HOME/lib
echo &#39;{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:postgresql://localhost:5432/test&quot;,
        &quot;user&quot; : &quot;fred&quot;,
        &quot;password&quot; : &quot;secret&quot;,
        &quot;sql&quot; : &quot;select * from orders&quot;,
        &quot;index&quot; : &quot;myindex&quot;,
        &quot;type&quot; : &quot;mytype&quot;
    }
}&#39; | java \
       -cp &quot;${lib}/*&quot; \
       -Dlog4j.configurationFile=${bin}/log4j2.xml \
       org.xbib.tools.Runner \
       org.xbib.tools.JDBCImporter
</code></p></li><li><p>Check log messages</p><p>In case the user does not exist, Elasticsearch will log a message.</p></li></ol>

<h2>MS SQL Server</h2>

<ol><li><p>Download Elasticsearch</p></li><li><p>Install Elasticsearch</p><p>Follow instructions on https://www.elastic.co/products/elasticsearch</p></li><li><p>Install JDBC importer</p><p><code>wget http://xbib.org/repository/org/xbib/elasticsearch/importer/elasticsearch-jdbc/&lt;version&gt;/elasticsearch-jdbc-&lt;version&gt;-dist.zip</code></p><p>(update version respectively)</p></li><li><p>Download SQL Server JDBC driver from <a href="http://msdn.microsoft.com/en-us/sqlserver/aa937724.aspx">the vendor</a></p></li><li><p>Copy driver into lib folder</p><p> cp SQLJDBC4.jar $JDBC<em>IMPORTER</em>HOME/lib</p></li><li><p>Set up the database you want to be indexed.
<a href="http://stackoverflow.com/questions/2388042/connect-to-sql-server-2008-with-tcp-ip">This includes allowing TCP/IP connections</a></p></li><li><p>Start Elasticsearch
<code>
./elasticsearch.bat
</code></p></li><li><p>Start JDBC importer
<code>
bin=$JDBC_IMPORTER_HOME/bin
lib=$JDBC_IMPORTER_HOME/lib
echo &#39;{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot;: {
        &quot;url&quot;:&quot;jdbc:sqlserver://localhost:1433;databaseName=ICFV&quot;,
        &quot;user&quot;:&quot;elasticsearch&quot;,
        &quot;password&quot;:&quot;elasticsearch&quot;,
        &quot;sql&quot;:&quot;select * from ScoreCards&quot;,
        &quot;index&quot; : &quot;myindex&quot;,
        &quot;type&quot; : &quot;mytype&quot;
    }
}&#39; | java \
       -cp &quot;${lib}/*&quot; \
       -Dlog4j.configurationFile=${bin}/log4j2.xml \
       org.xbib.tools.Runner \
       org.xbib.tools.JDBCImporter
</code></p></li><li><p>You should see messages from the importer in the logfile.</p></li></ol>

<h2>Index simple geo coordinates from MySQL in Elasticsearch</h2>

<ol><li><p>install MySQL e.g. in /usr/local/mysql</p></li><li><p>start MySQL on localhost:3306 (default)</p></li><li><p>prepare a &#39;test&#39; database in MySQL</p></li><li><p>create empty user &#39;&#39; with empty password &#39;&#39; (this user should exist as default user, otherwise set up a password and adapt the example)</p></li><li><p>execute SQL in &quot;geo.dump&quot; /usr/local/mysql/bin/mysql test &lt; src/test/resources/geo.dump</p></li><li><p>then run this script
<code>
curl -XDELETE &#39;localhost:9200/myjdbc&#39;
bin=$JDBC_IMPORTER_HOME/bin
lib=$JDBC_IMPORTER_HOME/lib
echo &#39;
{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
        &quot;user&quot; : &quot;&quot;,
        &quot;password&quot; : &quot;&quot;,
        &quot;locale&quot; : &quot;en_US&quot;,
        &quot;sql&quot; : [
            {
                &quot;statement&quot; : &quot;select \&quot;myjdbc\&quot; as _index, \&quot;mytype\&quot; as _type, name as _id, city, zip, address, lat as \&quot;location.lat\&quot;, lon as \&quot;location.lon\&quot; from geo&quot;
            }
        ],
        &quot;index&quot; : &quot;myjdbc&quot;,
        &quot;type&quot; : &quot;mytype&quot;,
        &quot;index_settings&quot; : {
            &quot;index&quot; : {
                &quot;number_of_shards&quot; : 1
            }
        },
        &quot;type_mapping&quot;: {
            &quot;mytype&quot; : {
                &quot;properties&quot; : {
                    &quot;location&quot; : {
                        &quot;type&quot; : &quot;geo_point&quot;
                    }
                }
            }
        }
    }
}&#39;  | java \
              -cp &quot;${lib}/*&quot; \
              -Dlog4j.configurationFile=${bin}/log4j2.xml \
              org.xbib.tools.Runner \
              org.xbib.tools.JDBCImporter
echo &quot;sleeping while importer should run...&quot;
sleep 10
curl -XGET &#39;localhost:9200/myjdbc/_refresh&#39;
curl -XPOST &#39;localhost:9200/myjdbc/_search?pretty&#39; -d &#39;
{
  &quot;query&quot;: {
     &quot;filtered&quot;: {
       &quot;query&quot;: {
          &quot;match_all&quot;: {
           }
       },
       &quot;filter&quot;: {
           &quot;geo_distance&quot; : {
               &quot;distance&quot; : &quot;20km&quot;,
               &quot;location&quot; : {
                    &quot;lat&quot; : 51.0,
                    &quot;lon&quot; : 7.0
                }
            }
        }
     }
   }
}&#39;
</code></p></li></ol>

<h2>Index simple geo coordinates from Postgres/PostGIS geometry field in Elasticsearch</h2>

<ol><li><p>install Postgres and PostGIS</p></li><li><p>start Postgres on localhost:5432 (default)</p></li><li><p>prepare a &#39;test&#39; database in Postgres, connect to the database using psql and create the PostGIS extension <code>CREATE EXTENSION POSTGIS</code></p></li><li><p>create user &#39;test&#39; with password &#39;test&#39;, quit psql</p></li><li><p>import geo table (includes geom field of type geometry) from &quot;geo.sql&quot; psql -U test -d test &lt; src/test/resources/geo.sql</p></li><li><p>then run this script. IMPORTANT: note the use of explicit rounding and scale parameter, by default PostGIS will output floats, these will cause you problems in your geom_point in Elasticsearch unless you use specific casts, you have been warned!
<code>
curl -XDELETE &#39;localhost:9200/myjdbc&#39;
bin=$JDBC_IMPORTER_HOME/bin
lib=$JDBC_IMPORTER_HOME/lib
echo &#39;
{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:postgres://localhost:5432/test&quot;,
        &quot;user&quot; : &quot;test&quot;,
        &quot;password&quot; : &quot;test&quot;,
        &quot;locale&quot; : &quot;en_GB&quot;,
        &quot;sql&quot; : &quot;select geonameid as _id, name, admin1_code, admin2_code, admin3_code, round(ST_Y(geom)::numeric,8) as \&quot;location.lat\&quot;, round(ST_X(geom)::numeric,8) as \&quot;location.lon\&quot; from geo&quot;,
        &quot;index&quot; : &quot;myjdbc&quot;,
        &quot;type&quot; : &quot;mytype&quot;,
        &quot;scale&quot; : 8,
        &quot;index_settings&quot; : {
            &quot;index&quot; : {
                &quot;number_of_shards&quot; : 1
            }
        },
        &quot;type_mapping&quot;: {
            &quot;mytype&quot; : {
                &quot;properties&quot; : {
                    &quot;location&quot; : {
                        &quot;type&quot; : &quot;geo_point&quot;
                    }
                }
            }
        }
    }
}&#39;  | java \
              -cp &quot;${lib}/*&quot; \
              -Dlog4j.configurationFile=${bin}/log4j2.xml \
              org.xbib.tools.Runner \
              org.xbib.tools.JDBCImporter
echo &quot;sleeping while importer should run...&quot;
sleep 10
curl -XGET &#39;localhost:9200/myjdbc/_refresh&#39;
curl -XPOST &#39;localhost:9200/myjdbc/_search?pretty&#39; -d &#39;
{
  &quot;query&quot;: {
     &quot;filtered&quot;: {
       &quot;query&quot;: {
          &quot;match_all&quot;: {
           }
       },
       &quot;filter&quot;: {
           &quot;geo_distance&quot; : {
               &quot;distance&quot; : &quot;20km&quot;,
               &quot;location&quot; : {
                    &quot;lat&quot; : 51.477347,
                    &quot;lon&quot; : -0.000850
                }
            }
        }
     }
   }
}&#39;
</code></p></li></ol>

<h2>Geo shapes</h2>

<p>The JDBC importer understands WKT http://en.wikipedia.org/wiki/Well-known_text 
&quot;POINT&quot; and &quot;POLYGON&quot; formats and converts them to GeoJSON.</p>

<p>With MySQL, the <code>astext</code> function can format WKT from columns of type <code>geometry</code>.</p>

<p>Example:</p>

<pre><code>mysql -u root test &lt;&lt;EOT
drop table if exists test.geom;
create table test.geom (
	id integer,
	g geometry
);
set @g = &#39;POLYGON((0 0,10 0,10 10,0 10,0 0),(5 5,7 5,7 7,5 7, 5 5))&#39;;
insert into test.geom values (0, GeomFromText(@g));
EOT

curl -XDELETE &#39;localhost:9200/myjdbc&#39;
echo &#39;
{
	&quot;type&quot; : &quot;jdbc&quot;,
	&quot;jdbc&quot; : {
		&quot;url&quot; : &quot;jdbc:mysql://localhost:3306/test&quot;,
		&quot;user&quot; : &quot;&quot;,
		&quot;password&quot; : &quot;&quot;,
		&quot;locale&quot; : &quot;en_US&quot;,
		&quot;sql&quot; : &quot;select \&quot;myjdbc\&quot; as _index, \&quot;mytype\&quot; as _type, id as _id, astext(g) as polygon from geom&quot;,
		&quot;elasticsearch&quot; : {
			 &quot;cluster&quot; : &quot;elasticsearch&quot;,
			 &quot;host&quot; : &quot;localhost&quot;,
			 &quot;port&quot; : 9300
		},
		&quot;index&quot; : &quot;myjdbc&quot;,
		&quot;type&quot; : &quot;mytype&quot;,
		&quot;index_settings&quot; : {
			&quot;index&quot; : {
				&quot;number_of_shards&quot; : 1
			}
		},
		&quot;type_mapping&quot;: {
			&quot;mytype&quot; : {
				&quot;properties&quot; : {
					&quot;polygon&quot; : {
						&quot;type&quot; : &quot;geo_shape&quot;,
						&quot;tree&quot; : &quot;quadtree&quot;
					}
				}
			}
		}
	}
}
&#39; | java \
	-cp &quot;${lib}/*&quot; \
	-Dlog4j.configurationFile=${bin}/log4j2.xml \
	org.xbib.tools.Runner \
	org.xbib.tools.JDBCImporter</code></pre>

<h2>Oracle column name 30 character limit</h2>

<p>Oracle imposes a 30 character limit on column name aliases. This makes it sometimes hard to define columns names for
Elasticsearch field names. For this, a column name map can be used like this:</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:oracle:thin:@//localhost/sid&quot;,
        &quot;user&quot; : &quot;user&quot;,
        &quot;password&quot; : &quot;password&quot;,
        &quot;sql&quot; : &quot;select or_id as \&quot;_id\&quot;, or_tan as \&quot;o.t\&quot;, or_status as \&quot;o.s\&quot;, stages.* from orders, stages where or_id = st_or_id and or_seqno = st_seqno&quot;,
        &quot;column_name_map&quot; : {
           &quot;o&quot; : &quot;order&quot;,
           &quot;t&quot; : &quot;transaction_id&quot;,
           &quot;s&quot; : &quot;status&quot;
        }
    }
}</code></pre>

<h2>Connection properties for JDBC driver</h2>

<p>For some JDBC drivers, advanced parameters can be passed that are not specified in the driver URL, 
but in the JDBC connection properties. You can specifiy connection properties like this:</p>

<pre><code>{
    &quot;type&quot; : &quot;jdbc&quot;,
    &quot;jdbc&quot; : {
        &quot;url&quot; : &quot;jdbc:oracle:thin:@//localhost:1521/sid&quot;,
        &quot;user&quot; : &quot;user&quot;,
        &quot;password&quot; : &quot;password&quot;,
        &quot;sql&quot; : &quot;select ... from ...&quot;,
        &quot;connection_properties&quot; : {
            &quot;oracle.jdbc.TcpNoDelay&quot; : false,
            &quot;useFetchSizeWithLongColumn&quot; : false,
            &quot;oracle.net.CONNECT_TIMEOUT&quot; : 10000,
            &quot;oracle.jdbc.ReadTimeout&quot; : 50000
        }
    }
}</code></pre>